# A Comparision Between Locally Weighted Regression and Random Forest
This project will compare the performance of random forest regressor and locally weighted regression on smaples with one variable. The dataset I choose for the purpose of this project is Boston Housing Prices. Mean Square Error is used to evaluate the accuracy of the two models. I will do a K fold cross validation on the result of the predictions made by each model. 

## Random Forest
Random Forest is an ensemble model made up of an ensemble of decision trees. It was first introduced in 1995 by Ho to address the problem of overfitting in deelpy grown decision tree model and increase the overal accuracy of both training and testing data. To achieve this process, we need to generate a series of data based on the original dataset. The data generated by this method, bootstrapping, looks similar like the original data. After new sets of data are produced, decision trees are fitted to the datasets. For regression problems, the overall random forest outcome will take the average outcome of all decision trees.

For the purpose of this project, we will use random forest model from scikit learn library.

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error as mse
from sklearn.preprocessing import StandardScaler as scale
```
Import the data.

We will use lstat as the independent varible (x) and cmedv(corrected median values of owner-occupied housing in USD 1000) as the dependent variable(y).
```
data = pd.read_csv('...path/BostonHousingPrices.csv')
x = data['lstat'].values
y = data['cmedv'].values
```

First, I want to plot x, y values in a 2-D space
```
plt.scatter(x,y)
plt.show()
```



