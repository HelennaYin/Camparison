# A Comparision Between Locally Weighted Regression and Random Forest
This project will compare the performance of random forest regressor and locally weighted regression on smaples with one variable. The dataset I choose for the purpose of this project is Boston Housing Prices. Mean Square Error is used to evaluate the accuracy of the two models. I will do a K fold cross validation on the result of the predictions made by each model. 

## Random Forest
Random Forest is an ensemble model made up of an ensemble of decision trees. It was first introduced in 1995 by Ho to address the problem of overfitting in deelpy grown decision tree model and increase the overal accuracy of both training and testing data. To achieve this process, we need to generate a series of data based on the original dataset. The data generated by this method, bootstrapping, looks similar like the original data. After new sets of data are produced, decision trees are fitted to the datasets. For regression problems, the overall random forest outcome will take the average outcome of all decision trees.

For the purpose of this project, we will use random forest model from scikit learn library.

```
from sklearn.ensemble import RandomForestRegressor as RFR
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error as mse
from sklearn.preprocessing import StandardScaler as scale
```
Import the data.

We will use lstat as the independent varible (x) and cmedv(corrected median values of owner-occupied housing in USD 1000) as the dependent variable(y).
```
data = pd.read_csv('...path/BostonHousingPrices.csv')
x = data['lstat'].values
y = data['cmedv'].values
```

First, I want to plot x, y values in a 2-D space
```
plt.scatter(x,y)
plt.show()
```
<img src="sample.png" width="400" height="250" alt="hi" class="inline"/>


To compare the accuracy of the models, I will use the crossvalidated mean  square error between y and the predicted values of y. 
```
def DoKFold(model,X,y):
    
    mse_list = []
    kf = KFold(n_splits=10,random_state=123,shuffle = True)

    for idxtrain,idxtest in kf.split(X):
        Xtrain = X[idxtrain]
        Xtest = X[idxtest]
        ytrain = y[idxtrain]
        ytest = y[idxtest]
      
        model.fit(Xtrain,ytrain)
        mse_list.append(mse(ytest,model.predict(Xtest)))
        
    return np.mean(mse_list)
X = x.reshape(-1,1)
mse_ = []
model = RFR(max_depth = 3,random_state=146)
mse_.append(DoKFold(model,X,y))
y_est = model.predict(X)
print(mse_)
```
**[27.286248839501887]**
The mean square error of yhat estimated by random forest regressor and sample y is 27.29

The following lines of code will draw a figure of sample data points and the curve of estimated y.
```
x_sorted = np.sort(x)
yest_sorted = model.predict(x_sorted.reshape(-1,1))
plt.figure(figsize=(8,5))
plt.scatter(x,y,facecolors = 'none', edgecolor = 'darkblue', label = 'Sample Data Points')
plt.plot(x_sorted,yest_sorted,color='red',lw=2,label = 'Random Forest')
plt.legend()
plt.title('Data Points and Random Forest Learner')
plt.show()x_sorted = np.sort(x)
yest_sorted = model.predict(x_sorted.reshape(-1,1))
plt.figure(figsize=(8,5))
plt.scatter(x,y,facecolors = 'none', edgecolor = 'darkblue', label = 'Sample Data Points')
plt.plot(x_sorted,yest_sorted,color='red',lw=2,label = 'Random Forest')
plt.legend()
plt.title('Data Points and Random Forest Learner')
plt.show()
```
<img src="RF.png" width="800" height="500" alt="hi" class="inline"/>

## Locally Weighted Regression
Locally Weighted Regression, or LOESS, is a non-parametric regression model.

For each data points, we create a weight. The weight is decided by the data points near the desired data points. To conduct locally weighted regression analysis, we need to set up the 'neighborhood' of nearby data.

We sent the Euclidean distances between the desired observation and points in the neighborhood into a weighting function. The further the data point from the desired point, the less the weight is. We denote the output weight as W.

For a linear regression algorithm, we compute  ùõΩ  using the normal equation:

<img src="equation1.png" width="200" height="50" alt="hi" class="inline"/>

ùõΩ is a vector of all parameters, X is a matrix with all the observations.

For locally weighted regression, we will add the weighting terms into the noraml equation:

<img src="equation2.png" width="250" height="50" alt="hi" class="inline"/>

W is a matrix with all the weights on the main diagonal. All other elements of W are 0.

The estimated y will be:

<img src="equation3.png" width="250" height="50" alt="hi" class="inline"/>
