# A Comparision Between Locally Weighted Regression and Random Forest
This project will compare the performance of random forest regressor and locally weighted regression on smaples with one variable. The dataset I choose for the purpose of this project is Boston Housing Prices. Mean Square Error is used to evaluate the accuracy of the two models. I will do a K fold cross validation on the result of the predictions made by each model. 

## Random Forest
Random Forest is an ensemble model made up of an ensemble of decision trees. It was first introduced in 1995 by Ho to address the problem of overfitting in deelpy grown decision tree model and increase the overal accuracy of both training and testing data. To achieve this process, we need to generate a series of data based on the original dataset. The data generated by this method, bootstrapping, looks similar like the original data. After new sets of data are produced, decision trees are fitted to the datasets. For regression problems, the overall random forest outcome will take the average outcome of all decision trees.

For the purpose of this project, we will use random forest model from scikit learn library.

```
from sklearn.ensemble import RandomForestRegressor as RFR
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error as mse
from sklearn.preprocessing import StandardScaler as scale
```
Import the data.

We will use lstat as the independent varible (x) and cmedv(corrected median values of owner-occupied housing in USD 1000) as the dependent variable(y).
```
data = pd.read_csv('...path/BostonHousingPrices.csv')
x = data['lstat'].values
y = data['cmedv'].values
```

First, I want to plot x, y values in a 2-D space
```
plt.scatter(x,y)
plt.show()
```
<img src="sample.png" width="400" height="250" alt="hi" class="inline"/>


To compare the accuracy of the models, I will use the crossvalidated mean  square error between y and the predicted values of y. 
```
def DoKFold(model,X,y):
    
    mse_list = []
    kf = KFold(n_splits=10,random_state=123,shuffle = True)

    for idxtrain,idxtest in kf.split(X):
        Xtrain = X[idxtrain]
        Xtest = X[idxtest]
        ytrain = y[idxtrain]
        ytest = y[idxtest]
      
        model.fit(Xtrain,ytrain)
        mse_list.append(mse(ytest,model.predict(Xtest)))
        
    return np.mean(mse_list)
X = x.reshape(-1,1)
mse_ = []
model = RFR(max_depth = 3,random_state=146)
mse_.append(DoKFold(model,X,y))
y_est = model.predict(X)
print(mse_)
```
**[27.286248839501887]**
The mean square error of yhat estimated by random forest regressor and sample y is 27.29

The following lines of code will draw a figure of sample data points and the curve of estimated y.
```
x_sorted = np.sort(x)
yest_sorted = model.predict(x_sorted.reshape(-1,1))
plt.figure(figsize=(8,5))
plt.scatter(x,y,facecolors = 'none', edgecolor = 'darkblue', label = 'Sample Data Points')
plt.plot(x_sorted,yest_sorted,color='red',lw=2,label = 'Random Forest')
plt.legend()
plt.title('Data Points and Random Forest Learner')
plt.show()x_sorted = np.sort(x)
yest_sorted = model.predict(x_sorted.reshape(-1,1))
plt.figure(figsize=(8,5))
plt.scatter(x,y,facecolors = 'none', edgecolor = 'darkblue', label = 'Sample Data Points')
plt.plot(x_sorted,yest_sorted,color='red',lw=2,label = 'Random Forest')
plt.legend()
plt.title('Data Points and Random Forest Learner')
plt.show()
```
<img src="RF.png" width="800" height="500" alt="hi" class="inline"/>

## Locally Weighted Regression
Locally Weighted Regression, or LOESS, is a non-parametric regression model.

For each data points, we create a weight. The weight is decided by the data points near the desired data points. To conduct locally weighted regression analysis, we need to set up the 'neighborhood' of nearby data.

We sent the Euclidean distances between the desired observation and points in the neighborhood into a weighting function. The further the data point from the desired point, the less the weight is. We denote the output weight as W.

For a linear regression algorithm, we compute  ùõΩ  using the normal equation:

<img src="equation1.png" width="200" height="50" alt="hi" class="inline"/>

ùõΩ is a vector of all parameters, X is a matrix with all the observations.

For locally weighted regression, we will add the weighting terms into the noraml equation:

<img src="equation2.png" width="250" height="50" alt="hi" class="inline"/>

W is a matrix with all the weights on the main diagonal. All other elements of W are 0.

The estimated y will be:

<img src="equation3.png" width="250" height="50" alt="hi" class="inline"/>

In the following part, I will build a locally weighted regression algorithm.

First, we need to construct the weighting function, or kernel. The most common kernels are tricubic kernel. In the following lines of kernel function, we also specify the size of the 'neighborhood' of the x observations that are used to build the local regression model. Here, we will threshold the distance of 1, meaning that we only use the points which its distance from the desired data points is less than one to calculate the weights.

```
def tricubic(x):
  return np.where(np.abs(x)>1,0,(1-np.abs(x)**3)**3)
```

The following lines of code is the algorithm of one-variable locally weighted regression. We will use the hyperparameter tau to specify the bandwidth of the kernel. To find a better learner, we will adjust the tau value to find the optimal hyperparameter later in this section. The function will initiate the weight by inputing the euclidean distances of the desired x(the ith observation in the whole sample) to other observations to the kernel function and divide it with the hyperparameter. Then, it will loop through x and predict the corresponding y with locally weighted regression. The function will return all estimated y.  

```
from scipy import linalg
from scipy.interpolate import interp1d
def lowess_reg(x, y, xnew, kern, tau):
    n = len(x)
    yest = np.zeros(n)

    w = np.array([kern((x - x[i])/(2*tau)) for i in range(n)])     
    
    for i in range(n):
        weights = w[:, i]
        b = np.array([np.sum(weights * y), np.sum(weights * y * x)])
        A = np.array([[np.sum(weights), np.sum(weights * x)],
                    [np.sum(weights * x), np.sum(weights * x * x)]])
        theta, res, rnk, s = linalg.lstsq(A, b)
        yest[i] = theta[0] + theta[1] * x[i] 
    f = interp1d(x, yest,fill_value='extrapolate')
    return f(xnew)
```
We can now test the algorithm on the dataset we imported earlier. We will use cross-validation to evaluate the accuracy of this learning algorithm. Also, the loop over tau_range will find the optimal tau that yields the smallest mean square error of predicted y and sample y.

```
def DoKFold(X,y,tau,kern):
    
    mse_list = []
    kf = KFold(n_splits=10,random_state=123,shuffle = True)

    for idxtrain,idxtest in kf.split(X):
        Xtrain = X[idxtrain]
        Xtest = X[idxtest]
        ytrain = y[idxtrain]
        ytest = y[idxtest]

        yest = lowess_reg(Xtrain, ytrain, Xtest,kern, tau)
        mse_list.append(mse(ytest,yest))
    return np.mean(mse_list)
 
 tau_range = np.linspace(0.1,2,20)
 
MSE_Lowess = []
for i in tau_range:
  MSE_Lowess.append(DoKFold(x,y,i,tricubic))

print(tau_range[np.where(MSE_Lowess==np.min(MSE_Lowess))])
print(np.min(MSE_Lowess))
```
**array([1.5])**

**26.97001081825933**

The cross validated mean square error of y predicted by locally weighted regression and sample data is 26.97. Compared to that of y predicted by random forest, 27.29, locally weighted regressor achieved the better result.

The following figure shows the curve of predicted y of locally weighted regressor. 

```
x_sorted = np.sort(x)
yest_sorted = lowess_reg(x, y, x_sorted, tricubic, 1.5)

plt.figure(figsize=(8,5))
plt.scatter(x,y,facecolors = 'none', edgecolor = 'darkblue', label = 'Sample Data Points')
plt.plot(x_sorted,yest_sorted,color='red',lw=2,label = 'Locally Weighted Regression')
plt.legend()
plt.title('Data Points and Locally Weighted Regression')
plt.show()
```
<img src="LWR.png" width="800" height="500" alt="hi" class="inline"/>

## Conclusion
In this project, we compare the model accuracy of random forest and locally weighted regression. With a real dataset, it is validated that locally weighted regression will achieve a better result in cross validated mean square error.

References:
https://towardsdatascience.com/loess-373d43b03564
https://xavierbourretsicotte.github.io/loess.html
